import os
import json
import chevron
import random
from typing import List, Dict, Union
import copy 

from .tiny_factory import TinyFactory
from tinytroupe.factory import logger
from tinytroupe import openai_utils
from tinytroupe.agent import TinyPerson
import tinytroupe.utils as utils
from tinytroupe.control import transactional
from tinytroupe.factory import default

import concurrent.futures
import threading

import math

# to protect from race conditions when generating agents in parallel
concurrent_agent_generataion_lock = threading.Lock()


class TinyPersonFactory(TinyFactory):

    # keep track of all the names generated by all the factories, to ensure they are globally unique.
    all_unique_names=[]

    def __init__(self, sampling_space_description:str=None, total_population_size:int=None, context:str=None, simulation_id:str=None):
        """
        Initialize a TinyPersonFactory instance.

        Args:
            sampling_space_description (str, optional): The description of the sampling space. Defaults to None. If this is
               specified, then population_size must also be specified.
            population_size (int, optional): The size of the population to sample from. Defaults to None.
            context (str): The context text used to generate the TinyPerson instances.
            simulation_id (str, optional): The ID of the simulation. Defaults to None.
        """
        super().__init__(simulation_id)
        self.person_prompt_template_path = os.path.join(os.path.dirname(__file__), 'prompts/generate_person.mustache')
        self.context_text = context
        self.sampling_space_description = sampling_space_description
        self.population_size = total_population_size
        
        self.sampling_dimensions = None
        self.sampling_plan = None
        self.remaining_characteristics_sample = None
        self.remaining_unique_names = None

        self.generated_minibios = [] # keep track of the generated persons. We keep the minibio to avoid generating the same person twice.
        self.generated_names = []

    # TODO obsolete?
    @staticmethod
    def generate_person_factories(number_of_factories, generic_context_text):
        """
        Generate a list of TinyPersonFactory instances using OpenAI's LLM.

        Args:
            number_of_factories (int): The number of TinyPersonFactory instances to generate.
            generic_context_text (str): The generic context text used to generate the TinyPersonFactory instances.

        Returns:
            list: A list of TinyPersonFactory instances.
        """
        
        logger.info(f"Starting the generation of the {number_of_factories} person factories based on that context: {generic_context_text}")
        
        system_prompt = open(os.path.join(os.path.dirname(__file__), 'prompts/generate_person_factory.md')).read()

        messages = []
        messages.append({"role": "system", "content": system_prompt})

        user_prompt = chevron.render("Please, create {{number_of_factories}} person descriptions based on the following broad context: {{context}}", {
            "number_of_factories": number_of_factories,
            "context": generic_context_text
        })

        messages.append({"role": "user", "content": user_prompt})

        response = openai_utils.client().send_message(messages)

        if response is not None:
            result = utils.extract_json(response["content"])

            factories = []
            for i in range(number_of_factories):
                logger.debug(f"Generating person factory with description: {result[i]}")
                factories.append(TinyPersonFactory(result[i]))

            return factories

        return None

    @staticmethod
    def create_factory_from_demography(demography_description_or_file_path:Union[str, dict],  population_size:int, context:str=None):
        """
        Create a TinyPersonFactory instance from a demography description, which can be wither given as a file path or a dictionary
        (but not both).

        Args:
            demography_description_or_file_path (Union[str, dict]): The demography description or the file path to the demography description.
            population_size (int): The size of the population to sample from.
            context (str, optional): Additional context text used to generate the TinyPerson instances. Defaults to None.            

        Returns:
            TinyPersonFactory: A TinyPersonFactory instance.
        """
        # read the demography description from a file or use the given dictionary
        if isinstance(demography_description_or_file_path, str):
            demography_description = json.loads(open(demography_description_or_file_path).read())
        elif isinstance(demography_description_or_file_path, dict):
            demography_description = demography_description_or_file_path
        else:
            raise ValueError("demography_description_or_file_path must be either a string or a dictionary.")

        if population_size is None:
            raise ValueError("population_size must be specified.")


        full_demography_description = \
        f"""
        # Sampling space specification

        The population described by the demographic data below. Make sure you consider very detailed, fine-grained, 
        characteristics of the individuals in the population.

        ## Directives
        Please follow these rules:
            - consider as many different population segments as possible, while always keeping proportions correct.For example, 
              instead of sampling 10 people from segment A and 5 from segment B, you can instead sample 2 from A, 1 from B, 
              and 7 others from other segments, provided the proportions are maintained correct and there are enough people to sample.
            - also use any built-in knowledge you might have of the populations in question to improve the sampling space, 
              provided this built-in knowledge does not conflict with the demographic data below.

        ## Demographic data
        {json.dumps(demography_description, indent=4)}
        """

        return TinyPersonFactory(context=context, 
                                 sampling_space_description=full_demography_description,
                                 total_population_size=population_size)
        

    def generate_person(self, 
                        agent_particularities:str=None, 
                        temperature:float=1.2, 
                        frequency_penalty:float=0.0,
                        presence_penalty:float=0.0, 
                        attempts:int=10,
                        post_processing_func=None) -> TinyPerson:
        """
        Generate a TinyPerson instance using OpenAI's LLM.

        Args:
            agent_particularities (str): The particularities of the agent.
            temperature (float): The temperature to use when sampling from the LLM.
            frequency_penalty (float): The frequency penalty to use when sampling from the LLM.
            presence_penalty (float): The presence penalty to use when sampling from the LLM.
            attempts (int): The number of attempts to generate a TinyPerson instance.
            post_processing_func (function): A function to apply to the generated agent after it is created.

        Returns:
            TinyPerson: A TinyPerson instance generated using the LLM.
        """

        logger.info(f"Starting the person generation based on that context: {self.context_text}")


        # are we going to use a pre-computed sample of characteristics too?
        if self.population_size is not None:
            
            with concurrent_agent_generataion_lock:
                if self.remaining_characteristics_sample is None:
                    # if the sample does not exist, we generate it here once.
                    self.initialize_sampling_plan()
            
            # CONCURRENT PROTECTION
            with concurrent_agent_generataion_lock:
                if len(self.remaining_characteristics_sample) == 0:
                    raise ValueError(f"No more agents to sample from the population, all of the {self.population_size} have been sampled.")
                else:
                    sampled_characteristics = self.remaining_characteristics_sample.pop()
                    fresh_agent_name = self.remaining_unique_names.pop()
            
            if agent_particularities is not None:
                agent_particularities =\
                    f"""
                        - Primary characteristics: {agent_particularities}

                        - Also use all the following additional characteristics that **do not** conflict with the primary ones:
                            * Demographics: {json.dumps(sampled_characteristics, indent=4)}
                            * Full name: {fresh_agent_name}

                        In case one of the additional characteristics conflicts with a primary one, please use the primary one
                        and ignore the additional one.

                    """
            else:
                agent_particularities = \
                    f"""
                    - Full name: {fresh_agent_name}

                    - Demographics: 
                         {json.dumps(sampled_characteristics, indent=4)}
                    """
        else: # no predefined population size, so we generate one-off agents.
            # CONCURRENT PROTECTION
            with concurrent_agent_generataion_lock:
                fresh_agent_name = self._unique_full_name(already_generated_names=TinyPersonFactory._all_used_and_precomputed_names(), 
                                                        context=self.context_text)

            if agent_particularities is not None:
                agent_particularities = \
                f"""

                - Primary characteristics: {agent_particularities}

                - Also use the following additional characteristics:
                    * Full name: {fresh_agent_name}

                In case the primary characteristics already specify a name, please use the primary name and ignore the additional one.
                """
            else:
                agent_particularities = f"Full name: {fresh_agent_name}"
                

    
        logger.info(f"Generating person with the following particularities: {agent_particularities}")

        # read example specs from files. 
        example_1 = json.load(open(os.path.join(os.path.dirname(__file__), '../examples/agents/Friedrich_Wolf.agent.json')))
        example_2 = json.load(open(os.path.join(os.path.dirname(__file__), '../examples/agents/Sophie_Lefevre.agent.json')))

        # We must include all agent names generated in the whole of the simulation, not only the ones generated by this factory,
        # since they all share the same name space.
        #
        # For the minibios, we only need to keep track of the ones generated by this factory, since they are unique to each factory
        # and are used to guide the sampling process.
        user_prompt = chevron.render(open(self.person_prompt_template_path).read(), {
            "context": self.context_text,
            "agent_particularities": agent_particularities,
            
            #Note that we need to dump them to JSON strings, to ensure we get double quotes,
            # and other formatting issues are avoided.
            "example_1": json.dumps(example_1["persona"], indent=4),
            "example_2": json.dumps(example_2["persona"], indent=4)
        })

        def aux_generate(attempt):
            messages = []
            messages += [{"role": "system", "content": "You are a system that generates specifications for realistic simulations of people. You follow the generation rules and constraints carefully."},
                        {"role": "user", "content": user_prompt}]
            

            # due to a technicality, we need to call an auxiliary method to be able to use the transactional decorator.
            message = self._aux_model_call(messages=messages, 
                                            temperature=temperature,
                                            frequency_penalty=frequency_penalty,
                                            presence_penalty=presence_penalty)

            if message is not None:
                result = utils.extract_json(message["content"])

                logger.debug(f"At attempt {attempt}, generated person parameters:\n{json.dumps(result, indent=4, sort_keys=True)}")

                # only accept the generated spec if the name is not already in use
                if not self._is_name_already_assigned(result["name"]):
                    return result
                else:
                    logger.info(f"Person with name {result['name']} was already generated, cannot be reused.")

            return None # no suitable agent was generated
        
        agent_spec = None
        attempt = 0
        while agent_spec is None and attempt < attempts:
            try:
                attempt += 1
                agent_spec = aux_generate(attempt=attempt)
            except Exception as e:
                logger.error(f"Error while generating agent specification: {e}")
        
        # create the fresh agent
        if agent_spec is not None:
            # the agent is created here. This is why the present method cannot be cached. Instead, an auxiliary method is used
            # for the actual model call, so that it gets cached properly without skipping the agent creation.
            
            # protect parallel agent generation
            with concurrent_agent_generataion_lock:
                person = TinyPerson(agent_spec["name"])
                self._setup_agent(person, agent_spec)
                if post_processing_func is not None:
                    post_processing_func(person)

                self.generated_minibios.append(person.minibio())
                self.generated_names.append(person.get("name"))

            return person
        else:
            logger.error(f"Could not generate an agent after {attempts} attempts.")
            return None
   
    def generate_people(self, number_of_people:int=None, 
                        agent_particularities:str=None, 
                        temperature:float=1.7, 
                        frequency_penalty:float=0.0,
                        presence_penalty:float=0.0,
                        attempts:int=10, 
                        post_processing_func=None,
                        parallelize=default["parallel_agent_generation"],
                        verbose:bool=False) -> list:
        """
        Generate a list of TinyPerson instances using OpenAI's LLM.

        Args:
            number_of_people (int): The number of TinyPerson instances to generate.
            agent_particularities (str): The particularities of the agent.
            temperature (float): The temperature to use when sampling from the LLM.
            frequency_penalty (float): The frequency penalty to use when sampling from the LLM.
            presence_penalty (float): The presence penalty to use when sampling from the LLM.
            attempts (int): The number of attempts to generate a TinyPerson instance.
            post_processing_func (function): A function to apply to the generated agent after it is created.
            parallalel_workers (int): The number of parallel workers to use when generating the people. Too many workers may cause the LLM to fail 
                due to throttling by the API.
            verbose (bool): Whether to print information about the generated people.

        Returns:
            list: A list of TinyPerson instances generated using the LLM.
        """

        if number_of_people is None:
            if self.population_size is None:
                raise ValueError("Either the number of people to generate or the population size must be specified.")
            number_of_people = self.population_size
            
        elif self.population_size is None:
            self.population_size = number_of_people

        elif number_of_people is not None and self.population_size is not None and number_of_people > self.population_size:
            raise ValueError(f"Cannot generate more people than the population size. Requested {number_of_people}, but the population size is {self.population_size}.")

        people = []
        if parallelize:
            people = self._generate_people_in_parallel(number_of_people=number_of_people, 
                                                        agent_particularities=agent_particularities, 
                                                        temperature=temperature, 
                                                        frequency_penalty=frequency_penalty,
                                                        presence_penalty=presence_penalty,
                                                        attempts=attempts, 
                                                        post_processing_func=post_processing_func,
                                                        verbose=verbose)
        else:
            people = self._generate_people_sequentially(number_of_people=number_of_people, 
                                                        agent_particularities=agent_particularities, 
                                                        temperature=temperature, 
                                                        frequency_penalty=frequency_penalty,
                                                        presence_penalty=presence_penalty,
                                                        attempts=attempts, 
                                                        post_processing_func=post_processing_func,
                                                        verbose=verbose)
        
        return people
        

    @transactional(parallel=True)
    def _generate_people_in_parallel(self, number_of_people:int=None, 
                        agent_particularities:str=None, 
                        temperature:float=1.5, 
                        frequency_penalty:float=0.0,
                        presence_penalty:float=0.0,
                        attempts:int=10, 
                        post_processing_func=None,
                        verbose:bool=False) -> list:
        people = []

        #
        # Concurrently generate the people. 
        # 
        # This vastly speeds up the process, but be careful with the number of workers, as too 
        # many may cause the LLM to fail due to throttling by the API.
        #

        # this is the function that will be executed in parallel
        def generate_person_wrapper(args):
            self, i, agent_particularities, temperature, frequency_penalty, presence_penalty, attempts, post_processing_func = args
            person = self.generate_person(agent_particularities=agent_particularities, 
                                        temperature=temperature, 
                                        frequency_penalty=frequency_penalty,
                                        presence_penalty=presence_penalty,
                                        attempts=attempts,
                                        post_processing_func=post_processing_func)
            return i, person

        with concurrent.futures.ThreadPoolExecutor() as executor:
            # we use a list of futures to keep track of the results
            futures = [
                executor.submit(generate_person_wrapper, (self, i, agent_particularities, temperature, frequency_penalty, presence_penalty, attempts, post_processing_func))
                for i in range(number_of_people)
            ]

            # we iterate over the futures as they are completed, and collect the results
            for future in concurrent.futures.as_completed(futures):
                i, person = future.result()
                if person is not None:
                    people.append(person)
                    info_msg = f"Generated person {i+1}/{number_of_people}: {person.minibio()}"
                    logger.info(info_msg)
                    if verbose:
                        print(info_msg)
                else:
                    logger.error(f"Could not generate person {i+1}/{number_of_people}.")
            
        return people
        
    # TODO still make this one available? 
    def _generate_people_sequentially(self, number_of_people:int=None, 
                        agent_particularities:str=None, 
                        temperature:float=1.5, 
                        frequency_penalty:float=0.0,
                        presence_penalty:float=0.0,
                        attempts:int=10, 
                        post_processing_func=None,
                        verbose:bool=False) -> list:
        """
        Generate the people sequentially, not in parallel. This is a simpler alternative.
        """
        people = []
        for i in range(number_of_people):
            person = self.generate_person(agent_particularities=agent_particularities, 
                          temperature=temperature, 
                          frequency_penalty=frequency_penalty,
                          presence_penalty=presence_penalty,
                          attempts=attempts,
                          post_processing_func=post_processing_func)
            if person is not None:
                people.append(person)
            info_msg = f"Generated person {i+1}/{number_of_people}: {person.minibio()}"
            logger.info(info_msg)
            if verbose:
                print(info_msg)
            else:
                logger.error(f"Could not generate person {i+1}/{number_of_people}.")
        
        return people

                                      
                                      

    def initialize_sampling_plan(self):
        """
        Computes a list of characteristics samples from a sampling space. 
        The sampling space is built from the given description through intermediary steps
        that actually build a sampling space and then randomly (and not via LLM) sample from it, thereby 
        ensuring that the sampling is not biased by the LLM (though the sampling space itself may be biased).

        All intermediary results are stored for later inspection.

        For example, given some n > 3 and a description like
           "Young Western people of different liberal professions."
        
        The final samples could be something like:
              [{"age": 25, "profession": "Architect", "country": "USA"}, 
                {"age": 27, "profession": "Lawyer", "country": "Canada"}, 
                ...
                {"age": 25, "profession": "Architect", "country": "USA"}]

        Args:
            n (int): The number of samples to generate.
            sampling_space_description (str): A description of the sampling space.

        """

        # a technicality - we need to use an auxiliary method to be able to use the transactional decorator effectively.
        return self._initialize_sampling_plan_transaction(n=self.population_size, description=self.sampling_space_description,context=self.context_text)
        
    @transactional()
    def _initialize_sampling_plan_transaction(self, n, description, context):
        """
        Auxiliary method to initialize the sampling plan. This is needed in order to be able to use the transactional decorator,
        due too a technicality - the method parameters must be such that when they change the transaction is nullified.
        """
        if self.remaining_characteristics_sample is None:
            # sampling dimensions
            self.sampling_dimensions = utils.try_function(lambda: self._compute_sampling_dimensions(sampling_space_description=description),
                                                    
                                                    # check that the result is a dict
                                                    postcond_func=lambda result: isinstance(result, dict))
            logger.debug(f"Sampling dimensions: {json.dumps(self.sampling_dimensions, indent=4)}")

            # sampling plan
            self.sampling_plan =  utils.try_function(lambda: self._compute_sample_plan(n=n, 
                                                        sampling_dimensions=self.sampling_dimensions),
                                                        
                                                        # checks that the plan is a list, not an empty dictionary, a number or a string
                                                        postcond_func = lambda result: isinstance(result, list) and len(result) > 0
                                                        )
            
            logger.debug(f"Sampling plan: {json.dumps(self.sampling_plan, indent=4)}")

            # flatten the sampling plan in concrete individual samples
            self.remaining_characteristics_sample =  utils.try_function(lambda: self._flatten_sampling_plan(sampling_plan=self.sampling_plan))
            logger.debug(f"Remaining characteristics sample: {json.dumps(self.remaining_characteristics_sample, indent=4)}")

            # sample the unique names all together. This seems more effective to ensure generated names are unique.
            naming_context =\
            f"""
            General context: {context}

            Population distribution description: {description}
            """
            
            self.remaining_unique_names =  self._unique_full_names(n=n, already_generated_names=TinyPersonFactory._all_used_and_precomputed_names(), context=naming_context)
            
            logger.debug(f"Unique names generated: {json.dumps(self.remaining_unique_names, indent=4)}")

            # check that the names are actually unique, considering that the return type is a list
            if len(set(self.remaining_unique_names)) != len(self.remaining_unique_names):
                raise ValueError("The names generated are not unique. This should not happen.")
            
        else:
            raise ValueError("Sampling plan already initialized. Cannot reinitialize it.")

    @classmethod
    def _all_used_and_precomputed_names(cls) -> list:
        """
        Returns all the names currently in use by agents and those pre-generated by all factories.
        """
        return TinyPerson.all_agents_names() + cls.all_unique_names
    
    def _is_name_globally_unique(self, name:str) -> bool:
        """
        Checks if a name is globally unique.
        """
        return name not in TinyPersonFactory.all_unique_names
    
    def _is_name_already_assigned(self, name:str) -> bool:
        """
        Checks if a name has already been assigned to a person.
        """
        return name in TinyPerson.all_agents_names()


    @utils.llm(temperature=1.2)
    def _compute_sampling_dimensions(self, sampling_space_description:str) -> dict:
        """
        Given a sampling description, computes the dimensions of the sampling space. 

        ## On your input

        Here's what to do depending on what the input sampling space description looks like:
          - Plain text: Abstract all the potential dimensions from the text. For example, if the text is "Young Western people of different liberal professions.", the dimensions could be "age", "profession", "country".
          - JSON: Do not use the JSON directly, but rather abstract the dimensions from it. Input JSONs can be obtained from various sources, and you should do your best to interpret them and produce a clean list of dimensions and their values, regardless of how complex the input JSON is. In particular, never use the JSON formatting itself as dimension names or values, but rather abstract the actual dimensions and values from it.
          - Tables or other structured data: Abstract the dimensions from the structured data. For example, if the data is in a table, you should extract the rows and columns and abstract the dimensions from them.
            
        
        ## On your output:
        You output a JSON containing a list of dimensions. Each output dimension **must** consist of:
          - a name;
          - EITHER a list of values OR a range of values (specified as a pair). 
        
        The output is formatted as a JSON object with the following structure:
        ```json	
        {
            "sampling_space_description": "A description of the sampling space.",
            "dimensions": [
                {
                    "name": "dimension_name_1",
                    "values": ["value1", "value2", ...]
                },
                {
                    "name": "dimension_name_2",
                    "range": [min, max]
                },
                ...
            ]
        }
        ```

        Unless values are necessarily numbers (e.g., age), they should be descriptive strings so that it is easy to understand what they mean.
        
        ## Example:
        Given the following INPUT sampling space description: "Young Western people of different liberal professions and social classes."

        The OUTPUT dimensions could be a dictionary with the following structure:
           ```json
           {
                "sampling_space_description": "Young Western people of different liberal professions and social classes.",
                "dimensions": [
                    {
                        "name": "age",
                        "range": [18, 30]
                    },
                    {
                        "name": "socioeconomic status",
                        "values": ["miserable", "poor", "middle class", "rich", "very rich"]
                    },
                    {
                        "name": "profession",
                        "values": ["Architect", "Lawyer", "Physician", "Accountant", ...]
                    },
                    {
                        "name": "country",
                            "values": ["USA", "Canada", "UK", "France", "Germany", "Italy", "Spain", "Portugal", "Netherlands", "Belgium", ...]
                        }
                    ]
            }
            ```
        
        Note in the example:
           - Age is given as anumeric range.
           - All other values are descriptive strings, human-friendly, no strange symbols or codes.
           - No value contains internal structure - just a name or short description.
        
        Args:
            sampling_space_description (str): A description of the sampling space.
        
        Returns:
            dict: A dictionary with the dimensions of the sampling space, as shown in the example above.
        """
        # the body of this method is handled by the @llm decorator.
    
    @utils.llm(temperature=1.5)
    def _compute_sample_plan(self, n:int, sampling_dimensions:dict) -> List[Dict[str, any]]:
        """
        Given a number n of elements to sample, and the dimensions of the sampling space, computes a *sample plan* of n elements from that space.  
        
        The input sampling dimensions have the following structure:

            ```json
                {
                    "sampling_space_description": "A description of the sampling space.",
                    "dimensions": [
                        {
                            "name": "dimension_name_1",
                            "values": ["value1", "value2", ...]
                        },
                        {
                            "name": "dimension_name_2",
                            "range": [min, max]
                        },
                        ...
                    ]
                }
            ```
        
        The *sample plan* to be generated is a list of *sampling directives*. Each *sampling directive* **always** consists of:
          - "sampled_values": a map from of dimensions from the sampling space to concrete values.
          - "quantity": to how many elements with those values should be sampled in total (note that this is always <= n).

        So your final output **MUST** follow this JSON structure:


            ```json
            [
                {
                    "sampled_values": {
                        "dimension_name_1": "value1",
                        "dimension_name_2": "value2",
                        ...
                    },
                    "quantity": number
                },
                
                {
                    "sampled_values": {
                        "dimension_name_1": "value1",
                        "dimension_name_2": "value2",
                        ...
                    },
                    "quantity": number
                },
                ...
            ]
            ```
        
        You should ensure that the quantity of requested samples in each *sampling directive* is proportional to their presumed size in the target population.
        That is to say, combinations of dimensions that are more common in the target population should be sampled more often.
        You can rely on your built-in knowledge or make educated guesses about such quantities and proportions to ensure that the sample is representative of the population.
        
        ## Example
        Given the following INPUT sampling dimensions:
        
        ```json
        {
            "sampling_space_description": "Young Western people of different liberal professions."
            "dimensions": [
                {
                    "name": "age",
                    "range": [18, 30]
                },
                {
                    "name": "profession",
                    "values": ["Architect", "Lawyer", "Physician", "Accountant", ...]
                },
                {
                    "name": "country",
                        "values": ["USA", "Canada", "UK", "France", "Germany", "Italy", "Spain", "Portugal", "Netherlands", "Belgium", ...]
                    }
                ]
           }

        An OUTPUT *sample plan* therefore is a list with the *sample plan*, where each element is a dictionary with a *sampling directive*. For example, an output based on the above dimensions could look like this:

        ```json 
        [
            {
                "sampled_values": {
                    "age": 25,
                    "profession": "Architect",
                    "country": "USA"
                },
                "quantity": 8
            },
            {
                "sampled_values": {
                    "age": 27,
                    "profession": "Lawyer",
                    "country": "Canada"
                },
                "quantity": 1
            },
            ...
        ]
        ```

        Note that the number of people from the countries are proportional to their populations (i.e., USA has a population around 8 times larger than Canada, so there are 8 times more people from the USA than from Canada in the sample).

        Args:
            n (int): The number of elements to sample in total. This number will be distributed across the dimensions proportionally
                to the presumed size the target population.
            sampling_dimensions (dict): The dimensions of the sampling space.
        
        Returns:
            list: A list with the *sample plan*, where each element is a dictionary with a *sampling directive*, as described above.
        """
        # the body of this method is handled by the @llm decorator.
    
    def _flatten_sampling_plan(self, sampling_plan:dict) -> list:
        """
        Given a sample plan, flattens it into a list of samples in such a way that the number of times each sample appears
        correspond to what was specified in the plan. The order is random to avoid bias.

        For example, an input sample plan could look like this:

        ```json 
        [
            {
                "sampled_values": {
                    "age": 25,
                    "profession": "Architect",
                    "country": "USA"
                },
                "quantity": 8
            },
            {
                "sampled_values": {
                    "age": 27,
                    "profession": "Lawyer",
                    "country": "Canada"
                },
                "quantity": 1
            },
            ...
        ]
        ```

        And the output would be something like:
        
        ```python
           [{"age": 25, "profession": "Architect", "country": "USA"}, 
            {"age": 27, "profession": "Lawyer", "country": "Canada"}, 
            ...
            {"age": 25, "profession": "Architect", "country": "USA"}]
        ```

        Args:
            sampling_plan (dict): The sample plan to flatten.

        Returns:
            list: A list of samples, where each sample is a dictionary with the sampled values.
        """
        samples = []
        for sample in sampling_plan:
            for _ in range(int(sample["quantity"])):
                samples.append(sample["sampled_values"])
        
        # randomize
        random.shuffle(samples) #inplace
        return samples

    @transactional()
    def _unique_full_name(self, already_generated_names: list, context:str=None) -> str:
        # a technicality - we need to use an auxiliary method to be able to use the transactional decorator effectively.
        # TODO update this somehow to avoid this cumbersome workaround.
        
        return self._aux_unique_full_name(already_generated_names=already_generated_names, context=context)


    @utils.llm(temperature=1.5, presence_penalty=0.5, frequency_penalty=0.5)
    def _aux_unique_full_name(self, already_generated_names: list, context:str=None) -> str:
        """
        Generates a unique full name for a person. The full name must not be in the list of already generated names.
        If necessary, you can generate a longer name to ensure it is new. You can also try tweaking the spelling or
        adding more surnames, so that the name is unique. However, the name **must** sound realistic and not be too far-fetched,
        not sound as if it was made up.
        
        The final result is only the name, nothing else:

           "Some name here"  ---> correct as it is just a name, nothing else
           "Some name here, because ..." ---> incorrect as it contains a reason
           "Some name here." ---> incorrect as it contains punctuation
           "Name: Some name here" ---> incorrect as it contains a label
           "Some name here, some other name here" ---> incorrect as it contains more than one name

        An optional context can be provided to guide the name generation, so that it is a realistic name for the context. For example, we know that different socio-economic classes have different naming conventions, so the context can be used to guide the name generation.

        Regarding the `already_generated_names`, you must:
            - NEVER generate a name that is already in the list of already generated names.
            - The names in `already_generated_names` ARE NOT examples of names to generate. They are just names that have already been generated and should not be repeated. You should generate new names regardless of the names in `already_generated_names`, the only constraint is that the new names should not be in the list of already generated names.
            - In particular, you are not to generate a similar name to that of those in `already_generated_names`, you are **not** building some kind of 
            logical sequence. Each name must be independent of the others.

        ## Example

          **Input:**
              already_generated_names: ["John Doe", "Jane Smith", "Alice Brown"]
              context: { 'age': 25, 'profession': 'Architect', 'country': 'USA' }
        
          **Output:**
              "Michael Johnson"

          Note that: 
            - The name "Michael Johnson" is not in the list of already generated names.
            - The ouput consists only of a name, nothing else.

        Args:
            already_generated_names (list): The list of already generated names.
            context (str): The context in which the name is being generated. This can be used to guide the name generation, so that it is a realistic name for the context.

        Returns:
            str: A unique full name for a person.
        """
        # the body of this method is handled by the @llm decorator
    
    @transactional()
    def _unique_full_names(self, n:int, already_generated_names: list, context:str=None) -> list:
        """
        Generates a list of n unique full names for people. The full names must not be in the list of already generated names.
        
        Args:
            n (int): The number of names to generate.
            already_generated_names (list): The list of already generated names.
            context (str): The context in which the names are being generated. This can be used to guide the name generation, so that it is a realistic name for the context.
        """

        logger.debug(f"Will generate {n} unique full names for people. Already generated names: {already_generated_names}")
        
        # let's split the n in smaller chunks to make the model's job easier
        chunk_size = 10
        chunks = math.ceil(n/chunk_size)

        forbidden_names = copy.deepcopy(already_generated_names)
        names = []

        max_iterations = chunks * 10 
        cur_iterations = 0

        while len(names) < n and cur_iterations < max_iterations:
            logger.debug(f"Currently already generated names: {forbidden_names}")
            logger.debug(f"Iteration {cur_iterations} - Generating {chunk_size} names. Currently have {len(names)} names. Max iterations to be allowed: {max_iterations}")
            temp_names = utils.try_function(\
                                lambda: \
                                    self._aux_unique_full_names(n=chunk_size , 
                                                                already_generated_names=forbidden_names,
                                                                context=context),

                                                                # checks that result and TinyPerson.all_agents_names() are disjoint sets,
                                                                # and result has n elements
                                                                postcond_func = lambda result: len(result) >= chunk_size,
                                retries=20)
            
            # add the new names to the names list, removing any duplicates from their combination
            names = list(set(names + temp_names))
            forbidden_names += names

            cur_iterations += 1
        
        if cur_iterations >= max_iterations:
            logger.error(f"Could not generate the requested number of names after {max_iterations} iterations. Moving on with the {len(names)} names generated.")
        
        TinyPersonFactory.all_unique_names += names

        return names

    @utils.llm(temperature=1.9, presence_penalty=1.8, frequency_penalty=1.8)
    def _aux_unique_full_names(self, n:int, already_generated_names: list, context:str=None) -> list:
        """
        Generates a list of n unique full names for people. The full names must not be in the list of already generated names.
        If necessary, you can generate longer names to ensure they are new. You can also try tweaking the spelling or
        adding more surnames, so that the names are unique. However, the names **must** sound realistic and not be too far-fetched,
        not sound as if they were made up.

        You **must** generate at least n names, and they must all be unique. If necessary, to ensure you get at least n names, you can try to generate more than n, but **never** less.
        
        The final result is only the list of names, nothing else:

           ["Some name here"]  ---> correct as it is just a list with a single name, nothing else
           ["Some name here, some other name here"] ---> correct as it is a list of names
           ["Some name here, because ..."] ---> incorrect as it contains a reason
           ["Some name here."] ---> incorrect as it contains punctuation
           ["Name: Some name here"] ---> incorrect as it contains a label

        An optional context can be provided to guide the name generation, so that it is a realistic name for the context. For example, we know that different socio-economic classes have different naming conventions, so the context can be used to guide the name generation.

        Regarding the `already_generated_names`, you must:
            - NEVER generate a name that is already in the list of already generated names.
            - The names in `already_generated_names` ARE NOT examples of names to generate. They are just names that have already been generated and should not be repeated. You should generate new names regardless of the names in `already_generated_names`, the only constraint is that the new names should not be in the list of already generated names.
            - In particular, you are not to generate a similar name to that of those in `already_generated_names`, you are **not** building some kind of  logical sequence. Each name must be independent of the others.

        ## Example

          **Input:**
              n: 6
              already_generated_names: ["John Doe", "Jane Smith", "Alice Brown"]
              context: "Young Americans of different liberal professions" 
          **Output:**
              ["Michael Johnson", "Sarah Williams", "David Gates", "Jennifer Davis", "Robert J. Wilson", "Anna Kerr"]

          Note that: 
            - The names are not in the list of already generated names.
            - The ouputs consist only of a list of names, nothing else.
            - The output length is exactly 6, which is the requested number of names. There could be a bit more names generated, but never less.

        Args:
            n (int): The number of names to generate.
            already_generated_names (list): The list of already generated names.
            context (str): The context in which the names are being generated. This can be used to guide the name generation, so that it is a realistic name for the context.

        Returns:
            list: A list of n unique full names for people. These names NEVER repeat names in the list of already generated names.
        """
        # the body of this method is handled by the @llm decorator. Below we provide a post-processing function that is
        # applied to the LLM output, to ensure that the names are unique.
              
        return lambda names: list(set(names))

    @transactional()
    def _aux_model_call(self, messages, temperature, frequency_penalty, presence_penalty):
        """
        Auxiliary method to make a model call. This is needed in order to be able to use the transactional decorator,
        due too a technicality - otherwise, the agent creation would be skipped during cache reutilization, and
        we don't want that.
        """
        return openai_utils.client().send_message(messages, 
                                                  temperature=temperature, 
                                                  frequency_penalty=frequency_penalty, 
                                                  presence_penalty=presence_penalty,
                                                  response_format={"type": "json_object"})
    
    @transactional()
    def _setup_agent(self, agent, configuration):
        """
        Sets up the agent with the necessary elements.
        """
        agent.include_persona_definitions(configuration)
        
        # does not return anything, as we don't want to cache the agent object itself.
    
